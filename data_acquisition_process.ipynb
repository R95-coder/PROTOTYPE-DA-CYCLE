{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bea9958-b405-4ddd-ac68-18f4b14a109b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b86321-860a-4289-b071-a0d9f84df0bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6916d7e9-5c00-4b21-be22-3efb9dc047c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataAcquisitionProcessor:\n",
    "    def __init__(self):\n",
    "        self.watermark_table = \"data_acquisition.watermark_control\"\n",
    "        self.pre_stage_table = \"data_acquisition.pre_stage_data\"\n",
    "        self.stage_table = \"data_acquisition.stage_data\"\n",
    "        self.reconciliation_table = \"data_acquisition.reconciliation_log\"\n",
    "        \n",
    "    def setup_database_and_tables(self):\n",
    "        \"\"\"Create database and required tables\"\"\"\n",
    "        logger.info(\"Setting up database and tables...\")\n",
    "        \n",
    "        # Create database\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS data_acquisition\")\n",
    "        \n",
    "        # Create watermark control table\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.watermark_table} (\n",
    "                source_system STRING,\n",
    "                table_name STRING,\n",
    "                last_processed_timestamp TIMESTAMP,\n",
    "                watermark_column STRING,\n",
    "                process_date DATE,\n",
    "                status STRING,\n",
    "                created_at TIMESTAMP,\n",
    "                updated_at TIMESTAMP\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create pre-stage table\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.pre_stage_table} (\n",
    "                id LONG,\n",
    "                customer_id STRING,\n",
    "                product_id STRING,\n",
    "                transaction_amount DOUBLE,\n",
    "                transaction_date TIMESTAMP,\n",
    "                status STRING,\n",
    "                source_system STRING,\n",
    "                load_timestamp TIMESTAMP,\n",
    "                batch_id STRING\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create stage table\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.stage_table} (\n",
    "                id LONG,\n",
    "                customer_id STRING,\n",
    "                product_id STRING,\n",
    "                transaction_amount DOUBLE,\n",
    "                transaction_date TIMESTAMP,\n",
    "                status STRING,\n",
    "                source_system STRING,\n",
    "                load_timestamp TIMESTAMP,\n",
    "                batch_id STRING,\n",
    "                processed_timestamp TIMESTAMP\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create reconciliation log table\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.reconciliation_table} (\n",
    "                batch_id STRING,\n",
    "                source_table STRING,\n",
    "                target_table STRING,\n",
    "                source_count LONG,\n",
    "                target_count LONG,\n",
    "                status STRING,\n",
    "                variance LONG,\n",
    "                process_timestamp TIMESTAMP,\n",
    "                remarks STRING\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "        logger.info(\"Database and tables setup completed.\")\n",
    "    \n",
    "    def get_watermark(self, source_system, table_name):\n",
    "        \"\"\"Get the last processed watermark\"\"\"\n",
    "        logger.info(f\"Getting watermark for {source_system}.{table_name}\")\n",
    "        \n",
    "        try:\n",
    "            watermark_df = spark.sql(f\"\"\"\n",
    "                SELECT last_processed_timestamp \n",
    "                FROM {self.watermark_table} \n",
    "                WHERE source_system = '{source_system}' \n",
    "                AND table_name = '{table_name}'\n",
    "                AND status = 'ACTIVE'\n",
    "                ORDER BY updated_at DESC \n",
    "                LIMIT 1\n",
    "            \"\"\")\n",
    "            \n",
    "            if watermark_df.count() > 0:\n",
    "                watermark = watermark_df.collect()[0]['last_processed_timestamp']\n",
    "                logger.info(f\"Found watermark: {watermark}\")\n",
    "                return watermark\n",
    "            else:\n",
    "                # Default watermark (24 hours ago)\n",
    "                default_watermark = datetime.now() - timedelta(days=1)\n",
    "                logger.info(f\"No watermark found, using default: {default_watermark}\")\n",
    "                return default_watermark\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting watermark: {e}\")\n",
    "            return datetime.now() - timedelta(days=1)\n",
    "    \n",
    "    def update_watermark(self, source_system, table_name, new_watermark, batch_id):\n",
    "        \"\"\"Update the watermark after successful processing\"\"\"\n",
    "        logger.info(f\"Updating watermark for {source_system}.{table_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Insert or update watermark\n",
    "            watermark_data = [(\n",
    "                source_system,\n",
    "                table_name,\n",
    "                new_watermark,\n",
    "                \"transaction_date\",\n",
    "                datetime.now().date(),\n",
    "                \"ACTIVE\",\n",
    "                datetime.now(),\n",
    "                datetime.now()\n",
    "            )]\n",
    "            \n",
    "            watermark_df = spark.createDataFrame(watermark_data, [\n",
    "                \"source_system\", \"table_name\", \"last_processed_timestamp\", \n",
    "                \"watermark_column\", \"process_date\", \"status\", \"created_at\", \"updated_at\"\n",
    "            ])\n",
    "            \n",
    "            watermark_df.write.mode(\"append\").insertInto(self.watermark_table)\n",
    "            \n",
    "            logger.info(\"Watermark updated successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating watermark: {e}\")\n",
    "    \n",
    "    def load_data_to_pre_stage(self, batch_id):\n",
    "        \"\"\"Load data from source to pre-stage table\"\"\"\n",
    "        logger.info(\"Loading data to pre-stage table...\")\n",
    "        \n",
    "        try:\n",
    "            # Read source data using S3A protocol\n",
    "            source_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3://da-process/sample_transactions.csv\")\n",
    "            \n",
    "            # Add metadata columns\n",
    "            pre_stage_df = source_df.withColumn(\"load_timestamp\", current_timestamp()) \\\n",
    "                                  .withColumn(\"batch_id\", lit(batch_id))\n",
    "            \n",
    "            # Write to pre-stage table\n",
    "            pre_stage_df.write.mode(\"append\").insertInto(self.pre_stage_table)\n",
    "            \n",
    "            source_count = pre_stage_df.count()\n",
    "            logger.info(f\"Loaded {source_count} records to pre-stage table\")\n",
    "            \n",
    "            return source_count\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data to pre-stage: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_data_to_stage(self, batch_id):\n",
    "        \"\"\"Load data from pre-stage to stage table with transformations\"\"\"\n",
    "        logger.info(\"Loading data to stage table...\")\n",
    "        \n",
    "        try:\n",
    "            # Read from pre-stage\n",
    "            pre_stage_df = spark.sql(f\"\"\"\n",
    "                SELECT * FROM {self.pre_stage_table} \n",
    "                WHERE batch_id = '{batch_id}'\n",
    "            \"\"\")\n",
    "            \n",
    "            # Apply transformations (example: data quality checks, business rules)\n",
    "            stage_df = pre_stage_df.filter(col(\"transaction_amount\") > 0) \\\n",
    "                                 .filter(col(\"customer_id\").isNotNull()) \\\n",
    "                                 .filter(col(\"product_id\").isNotNull()) \\\n",
    "                                 .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "            \n",
    "            # Write to stage table\n",
    "            stage_df.write.mode(\"append\").insertInto(self.stage_table)\n",
    "            \n",
    "            target_count = stage_df.count()\n",
    "            logger.info(f\"Loaded {target_count} records to stage table\")\n",
    "            \n",
    "            return target_count\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data to stage: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def perform_reconciliation(self, batch_id, source_count, target_count):\n",
    "        \"\"\"Perform reconciliation between source and target counts\"\"\"\n",
    "        logger.info(\"Performing reconciliation...\")\n",
    "        \n",
    "        try:\n",
    "            variance = source_count - target_count\n",
    "            \n",
    "            # Determine status based on variance\n",
    "            if variance == 0:\n",
    "                status = \"PASS\"\n",
    "                remarks = \"Record counts match perfectly\"\n",
    "            elif variance > 0:\n",
    "                status = \"WARN\"\n",
    "                remarks = f\"Source has {variance} more records than target\"\n",
    "            else:\n",
    "                status = \"FAIL\"\n",
    "                remarks = f\"Target has {abs(variance)} more records than source\"\n",
    "            \n",
    "            # Log reconciliation results\n",
    "            recon_data = [(\n",
    "                batch_id,\n",
    "                \"source_data\",\n",
    "                \"stage_data\",\n",
    "                source_count,\n",
    "                target_count,\n",
    "                status,\n",
    "                variance,\n",
    "                datetime.now(),\n",
    "                remarks\n",
    "            )]\n",
    "            \n",
    "            recon_df = spark.createDataFrame(recon_data, [\n",
    "                \"batch_id\", \"source_table\", \"target_table\", \"source_count\", \n",
    "                \"target_count\", \"status\", \"variance\", \"process_timestamp\", \"remarks\"\n",
    "            ])\n",
    "            \n",
    "            recon_df.write.mode(\"append\").insertInto(self.reconciliation_table)\n",
    "            \n",
    "            logger.info(f\"Reconciliation completed: {status} - {remarks}\")\n",
    "            \n",
    "            return {\n",
    "                \"status\": status,\n",
    "                \"source_count\": source_count,\n",
    "                \"target_count\": target_count,\n",
    "                \"variance\": variance,\n",
    "                \"remarks\": remarks\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during reconciliation: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_process_status(self, batch_id):\n",
    "        \"\"\"Get the current status of the data acquisition process\"\"\"\n",
    "        logger.info(f\"Getting process status for batch: {batch_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Get reconciliation status\n",
    "            status_df = spark.sql(f\"\"\"\n",
    "                SELECT \n",
    "                    batch_id,\n",
    "                    source_count,\n",
    "                    target_count,\n",
    "                    variance,\n",
    "                    status,\n",
    "                    remarks,\n",
    "                    process_timestamp\n",
    "                FROM {self.reconciliation_table}\n",
    "                WHERE batch_id = '{batch_id}'\n",
    "                ORDER BY process_timestamp DESC\n",
    "                LIMIT 1\n",
    "            \"\"\")\n",
    "            \n",
    "            if status_df.count() > 0:\n",
    "                return status_df.collect()[0].asDict()\n",
    "            else:\n",
    "                return {\"status\": \"NOT_FOUND\", \"message\": \"No reconciliation record found\"}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting process status: {e}\")\n",
    "            return {\"status\": \"ERROR\", \"message\": str(e)}\n",
    "    \n",
    "    def run_data_acquisition_cycle(self):\n",
    "        \"\"\"Run the complete data acquisition cycle\"\"\"\n",
    "        logger.info(\"Starting Data Acquisition Cycle...\")\n",
    "        \n",
    "        batch_id = f\"BATCH_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Setup\n",
    "            self.setup_database_and_tables()\n",
    "            \n",
    "            # Step 3: Get watermark\n",
    "            watermark = self.get_watermark(\"SYSTEM_A\", \"transactions\")\n",
    "            \n",
    "            # Step 4: Load to pre-stage\n",
    "            source_count = self.load_data_to_pre_stage(batch_id)\n",
    "            \n",
    "            # Step 5: Load to stage\n",
    "            target_count = self.load_data_to_stage(batch_id)\n",
    "            \n",
    "            # Step 6: Perform reconciliation\n",
    "            recon_result = self.perform_reconciliation(batch_id, source_count, target_count)\n",
    "            \n",
    "            # Step 7: Update watermark if successful\n",
    "            if recon_result[\"status\"] in [\"PASS\", \"WARN\"]:\n",
    "                new_watermark = datetime.now()\n",
    "                self.update_watermark(\"SYSTEM_A\", \"transactions\", new_watermark, batch_id)\n",
    "            \n",
    "            # Step 8: Get final status\n",
    "            final_status = self.get_process_status(batch_id)\n",
    "            \n",
    "            logger.info(\"Data Acquisition Cycle completed successfully\")\n",
    "            \n",
    "            return {\n",
    "                \"batch_id\": batch_id,\n",
    "                \"cycle_status\": \"COMPLETED\",\n",
    "                \"reconciliation\": recon_result,\n",
    "                \"final_status\": final_status\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data Acquisition Cycle failed: {e}\")\n",
    "            return {\n",
    "                \"batch_id\": batch_id,\n",
    "                \"cycle_status\": \"FAILED\",\n",
    "                \"error\": str(e)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238d51f9-b8e9-411d-b16e-b0dc9009b33e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # Initialize the processor\n",
    "processor = DataAcquisitionProcessor()\n",
    "\n",
    "# Run the data acquisition cycle\n",
    "result = processor.run_data_acquisition_cycle()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA ACQUISITION PROCESS RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Batch ID: {result['batch_id']}\")\n",
    "print(f\"Cycle Status: {result['cycle_status']}\")\n",
    "\n",
    "if result['cycle_status'] == 'COMPLETED':\n",
    "    recon = result['reconciliation']\n",
    "    print(f\"Reconciliation Status: {recon['status']}\")\n",
    "    print(f\"Source Count: {recon['source_count']}\")\n",
    "    print(f\"Target Count: {recon['target_count']}\")\n",
    "    print(f\"Variance: {recon['variance']}\")\n",
    "    print(f\"Remarks: {recon['remarks']}\")\n",
    "else:\n",
    "    print(f\"Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4142c489-fa54-4b5d-92c4-668e8993d3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display table contents for verification\n",
    "print(\"\\nTABLE CONTENTS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Pre-stage table\n",
    "print(\"PRE-STAGE TABLE:\")\n",
    "spark.sql(\"SELECT * FROM data_acquisition.pre_stage_data\").show(5)\n",
    "\n",
    "# Stage table\n",
    "print(\"STAGE TABLE:\")\n",
    "spark.sql(\"SELECT * FROM data_acquisition.stage_data\").show(5)\n",
    "\n",
    "# Reconciliation log\n",
    "print(\"RECONCILIATION LOG:\")\n",
    "spark.sql(\"SELECT * FROM data_acquisition.reconciliation_log\").show(5)\n",
    "\n",
    "# Watermark control\n",
    "print(\"WATERMARK CONTROL:\")\n",
    "spark.sql(\"SELECT * FROM data_acquisition.watermark_control\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a615fd12-1659-4b13-b774-a72edfeee013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_acquisition_process",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
